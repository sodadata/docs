{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dagster asset definition to export Soda check results to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "from dagster import get_dagster_logger, asset\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "\n",
    "url = 'https://cloud.soda.io/api/v1/scans'\n",
    "api_key_id = 'soda_api_key_id'\n",
    "api_key_secret = 'soda_api_key_secret')\n",
    "datasets_table = 'DATASETS_REPORT'\n",
    "checks_table = 'CHECKS_REPORT'\n",
    "\n",
    "# Connect to Redshift\n",
    "conn = psycopg2.connect(\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    host=host,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "get_dagster_logger().info(\"Starting SodaAPI to Redshift Tables.....\")\n",
    "datasets = []\n",
    "checks = []\n",
    "\n",
    "response_datasets = requests.get(\n",
    "        soda_cloud_url + '/api/v1/datasets?page=0', \n",
    "        auth=(soda_apikey , soda_apikey_secret)\n",
    "        )\n",
    "\n",
    "if response_datasets.status_code == 401 or response_datasets.status_code == 403:\n",
    "    get_dagster_logger().info(\"Unauthorized or Forbidden access. Please check your API keys and/or permissions in Soda.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Fetch all datasets\n",
    "if response_datasets.status_code == 200:\n",
    "    dataset_pages = response_datasets.json().get('totalPages')\n",
    "    \n",
    "    i = 0\n",
    "    while i < dataset_pages:\n",
    "        dq_datasets = requests.get(\n",
    "        soda_cloud_url + '/api/v1/datasets?page='+str(i), \n",
    "        auth=(soda_apikey , soda_apikey_secret))\n",
    "        \n",
    "        if dq_datasets.status_code == 200:\n",
    "            get_dagster_logger().info(\"Fetching all datasets on page: \"+str(i))\n",
    "            list = dq_datasets.json().get(\"content\")\n",
    "            datasets.extend(list)\n",
    "            i += 1\n",
    "        elif dq_datasets.status_code == 429:\n",
    "            get_dagster_logger().info(\"API Rate Limit reached when fetching datasets on page: \" +str(i)+ \". Pausing for 30 seconds.\")\n",
    "            time.sleep(30)\n",
    "        else:\n",
    "            get_dagster_logger().info(\"Error fetching datasets on page \"+str(i)+\". Status code:\", dq_datasets.status_code)\n",
    "else:\n",
    "    get_dagster_logger().info(\"Error fetching initial datasets. Status code:\", response_datasets.status_code)\n",
    "    sys.exit()\n",
    "\n",
    "df_datasets = pd.DataFrame(datasets)\n",
    "\n",
    "# Clean up df_datasets\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_datasets.insert(0, 'record_created', current_time)\n",
    "df_datasets['datasource_type'] = df_datasets['datasource'].apply(lambda x: x['type'] if x else None)\n",
    "df_datasets.drop(columns=['datasource'], inplace=True)\n",
    "\n",
    "# Fetch all checks\n",
    "response_checks = requests.get(\n",
    "    soda_cloud_url + '/api/v1/checks?size=100', \n",
    "    auth=(soda_apikey , soda_apikey_secret))\n",
    "\n",
    "if response_checks.status_code == 200:\n",
    "    check_pages = response_checks.json().get('totalPages')\n",
    "    \n",
    "    i = 0\n",
    "    while i < check_pages:\n",
    "        dq_checks = requests.get(\n",
    "            soda_cloud_url + '/api/v1/checks?size=100&page='+str(i), \n",
    "            auth=(soda_apikey , soda_apikey_secret))\n",
    "        \n",
    "        if dq_checks.status_code == 200:\n",
    "            get_dagster_logger().info(\"Fetching all checks on page \"+str(i))\n",
    "            check_list = dq_checks.json().get(\"content\")\n",
    "            checks.extend(check_list)\n",
    "            i += 1 \n",
    "        elif dq_checks.status_code == 429:\n",
    "            get_dagster_logger().info(\"API Rate Limit reached when fetching checks on page: \" +str(i)+ \". Pausing for 30 seconds.\")\n",
    "            time.sleep(30)\n",
    "        else:\n",
    "            get_dagster_logger().info(\"Error fetching checks on page \"+str(i)+\". Status code:\", dq_checks.status_code)\n",
    "else:\n",
    "    get_dagster_logger().info(\"Error fetching initial checks. Status code:\", response_checks.status_code)\n",
    "    sys.exit()\n",
    "\n",
    "df_checks = pd.DataFrame(checks)\n",
    "\n",
    "# Clean up checks_dataframe\n",
    "df_checks.insert(0, 'record_created', current_time)\n",
    "df_checks['dataset_id'] = df_checks['datasets'].apply(lambda x: x[0]['id'] if x else None)\n",
    "df_checks['dataset_name'] = df_checks['datasets'].apply(lambda x: x[0]['name'] if x else None)\n",
    "df_checks['dataset_url'] = df_checks['datasets'].apply(lambda x: x[0]['cloudUrl'] if x else None)\n",
    "df_checks['lastCheckResultValue'] = df_checks['lastCheckResultValue'].apply(lambda x: x.get('value') if isinstance(x, dict) and 'value' in x else (x.get('valueLabel') if isinstance(x, dict) and 'valueLabel' in x else x))\n",
    "df_checks['attributes'] = df_checks['attributes'].fillna({})\n",
    "df_checks['check_owner'] = df_checks['owner'].apply(lambda x: x.get('firstName', '') + ' ' + x.get('lastName', '') if isinstance(x, dict) else '')\n",
    "df_checks['owner_email'] = df_checks['owner'].apply(lambda x: x.get('email', '') if isinstance(x, dict) else '')\n",
    "df_checks['lastCheckResultValue'] = df_checks['lastCheckResultValue'].astype(str)\n",
    "    \n",
    "# Rename columns\n",
    "df_checks.rename(columns={'id': 'check_id'}, inplace=True)\n",
    "df_checks.rename(columns={'name': 'check_name'}, inplace=True)\n",
    "df_checks.rename(columns={'evaluationStatus': 'check_status'}, inplace=True)\n",
    "df_checks.rename(columns={'definition': 'check_definition'}, inplace=True)\n",
    "df_checks.rename(columns={'cloudUrl': 'check_url'}, inplace=True)\n",
    "\n",
    "# Add attribute names and values as separate columns\n",
    "for index, row in df_checks.iterrows():\n",
    "    attributes_dict = row['attributes']\n",
    "    for key in attributes_dict:\n",
    "        column_name = key.upper()\n",
    "        column_value = attributes_dict[key]\n",
    "        df_checks.at[index, column_name] = column_value\n",
    "\n",
    "# Drop original columns\n",
    "df_checks.drop(columns=['attributes', 'datasets', 'owner'], inplace=True)\n",
    "\n",
    "# Convert to str\n",
    "df_checks = df_checks.astype(str)\n",
    "\n",
    "df_checks.rename(columns={'column': 'check_column'}, inplace=True)\n",
    "df_checks.rename(columns={'group': 'check_group'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save in Redshift\n",
    "def execute_query(conn, query):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def table_exists(conn, table_name):\n",
    "    query = \"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s)\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (table_name.lower(),))\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "def create_table(conn, df, table_name):\n",
    "    columns_with_types = \", \".join([f\"{col} VARCHAR(500)\" for col in df.columns])  \n",
    "    query = f\"CREATE TABLE {table_name} ({columns_with_types})\"\n",
    "    execute_query(conn, query)\n",
    "\n",
    "\n",
    "\n",
    "def insert_into_table(conn, df, table_name):\n",
    "    columns = \", \".join(df.columns)\n",
    "    values = \", \".join([\"%s\"] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({values})\"\n",
    "    with conn.cursor() as cur:\n",
    "        for row in df.itertuples(index=False, name=None):\n",
    "            cur.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "# Check for existing table and add new columns if needed\n",
    "def update_table_structure(conn, df, table_name):\n",
    "    existing_columns_query = \"SELECT column_name FROM information_schema.columns WHERE table_name = %s\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(existing_columns_query, (table_name.lower(),))\n",
    "        existing_columns = [row[0] for row in cur.fetchall()]\n",
    "        new_columns = [col for col in df.columns if col.lower() not in existing_columns]\n",
    "        for col in new_columns:\n",
    "            alter_table_query = sql.SQL(\"ALTER TABLE {} ADD COLUMN {} VARCHAR\").format(\n",
    "                sql.Identifier(table_name),\n",
    "                sql.Identifier(col)\n",
    "            )\n",
    "            execute_query(conn, alter_table_query)\n",
    "            get_dagster_logger().info(f\"Added new column {col} to {table_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create or update datasets table\n",
    "if not table_exists(conn, datasets_table):\n",
    "    create_table(conn, df_datasets, datasets_table)\n",
    "else:\n",
    "    update_table_structure(conn, df_datasets, datasets_table)\n",
    "insert_into_table(conn, df_datasets, datasets_table)\n",
    "\n",
    "# Create or update checks table\n",
    "if not table_exists(conn, checks_table):\n",
    "    create_table(conn, df_checks, checks_table)\n",
    "else:\n",
    "    update_table_structure(conn, df_checks, checks_table)\n",
    "insert_into_table(conn, df_checks, checks_table)\n",
    "\n",
    "\n",
    "get_dagster_logger().info(f\"The following tables in Redshift were updated successfully: {datasets_table}, {checks_table}\")\n",
    "# Close connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
